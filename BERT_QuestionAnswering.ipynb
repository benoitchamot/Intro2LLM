{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with BERT\n",
    "Source: most of the code is copied or adapted from The Fuzzy Scientist's [LLMs Mastery: Complete Guide to Transformers & Generative AI](https://udemy.com/course/llms-mastery-complete-guide-to-transformers-generative-ai) course on Udemy. The course is a lot more extensive than what is presented here and should be followed to understand all the concepts and the full context.\n",
    "\n",
    "Source: Wikipedia article on James G. Blaine: https://en.wikipedia.org/wiki/James_G._Blaine (Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"James G. Blaine (1830–1893) was an American statesman and Republican politician who represented Maine in the U.S. House of Representatives from 1863 to 1876, serving as Speaker of the House from 1869 to 1875, and then in the Senate from 1876 to 1881.\n",
    "Born in Pennsylvania and a newspaper editor before entering politics, he twice served as the U.S. secretary of state, first in 1881 under President James A. Garfield and President Chester A. Arthur, and then from 1889 to 1892 under President Benjamin Harrison.\n",
    "Blaine unsuccessfully sought the Republican presidential nomination in 1876 and 1880.\n",
    "He gained the nomination in 1884, but in the election, he was narrowly defeated by Democratic nominee Grover Cleveland.\n",
    "A charismatic speaker in an age that prized oratory, Blaine was a leading Republican of the late 19th century and a champion of the party's moderate reformist faction, later known as the \"Half-Breeds\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"James G. Blaine (1830–1893) was an American statesman and Republican politician.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"When was James Blaine born?\"\n",
    "question_2 = \"What was James Blaine role from 1863 to 1876?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Import pre-trained model and tokeniser\n",
    "Model source: https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'deepset/bert-base-cased-squad2'\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the question\n",
    "Both the question and the context must be encoded together in a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences(sentences, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    for i in range(0, num_sentences, chunk_size-overlap):\n",
    "        chunk = sentences[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "\n",
    "    # Tokenise the question and context together in a single vector\n",
    "    inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "    # Disabling gradient calculation (torch.no_grad()) is useful for inference, when\n",
    "    # you are sure that you will not call Tensor.backward(). It will reduce memory\n",
    "    # consumption for computations that would otherwise have requires_grad=True.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Claculate the scores for the start and end of the answer\n",
    "    start_scores = softmax(outputs.start_logits)[0]\n",
    "    end_scores = softmax(outputs.end_logits)[0]\n",
    "\n",
    "    # Extract the start and end indices of the most likely answer\n",
    "    # selected by the model\n",
    "    start_index = np.argmax(start_scores)\n",
    "    end_index = np.argmax(end_scores)\n",
    "\n",
    "    # Basded on the start and end indices, retrieve:\n",
    "    # 1. the token ids\n",
    "    # 2. the tokens converted form the ids\n",
    "    # 3. a string converted from the tokens\n",
    "    answer_ids = inputs.input_ids[0][start_index:end_index+1]\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    # If the model is not able to answer the question based\n",
    "    # on the context, it will return the [CLS] token with a\n",
    "    # high confidence score.\n",
    "    # We replace this with a more human friendly response\n",
    "    if answer == tokenizer.cls_token:\n",
    "        answer = \"I cannot answer based on the provided context\"\n",
    "\n",
    "    # Calculate the averafe of the start and end token\n",
    "    # confidence scores to get an overall confidence\n",
    "    # Multiply by 100 to express the score as percent\n",
    "    confidence_score = 100*0.5*(start_scores[start_index] + end_scores[end_index])\n",
    "\n",
    "    return answer, confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: an American statesman and Republican politician (90.6%)\n",
      "valid answer\n",
      "Chunk 2: President (77.3%)\n",
      "valid answer\n",
      "Chunk 3: Grover Cleveland. (72.2%)\n",
      "valid answer\n",
      "Chunk 4: Republican of the late 19th century (64.3%)\n",
      "valid answer\n",
      "Chunk 5: a leading Republican of the late 19th century (73.9%)\n",
      "valid answer\n",
      "Q: 'who was James Blaine?'\n",
      "A: 'an American statesman and Republican politician'\n",
      "Confidence: 90.6%\n"
     ]
    }
   ],
   "source": [
    "# Select the question to ask\n",
    "question = \"who was James Blaine?\"\n",
    "\n",
    "# Split the context into separated sentences\n",
    "context_sentences = context.split('\\n')\n",
    "\n",
    "# Group the sentences into chunks of two sentences each and with an\n",
    "# overlap of one sentence (so no answers is left in between two chunks\n",
    "# if it spreads over two sentences)\n",
    "context_chunks = chunk_sentences(context_sentences, chunk_size=2, overlap=1)\n",
    "\n",
    "answers = {'answer': '', 'score': 0}\n",
    "\n",
    "for i, chunk in enumerate(context_chunks):\n",
    "    # Group the sentences in a chunk into a single string\n",
    "    sub_context = \"\\n\".join(chunk)\n",
    "\n",
    "    # Use the model to predict the answer based on each chunk as context\n",
    "    answer, confidence_score = answer_question(question, sub_context)\n",
    "    print(f\"Chunk {i+1}: {answer} ({confidence_score:.1f}%)\")\n",
    "\n",
    "    # Check if the answer is valid\n",
    "    if (answer != \"I cannot answer based on the provided context\"):\n",
    "        print('valid answer')\n",
    "        if confidence_score > answers['score']:\n",
    "            answers['answer'] = answer\n",
    "            answers['score'] = confidence_score\n",
    "\n",
    "# Print the question and answer:\n",
    "print(f\"Q: '{question}'\")\n",
    "print(f\"A: '{answers['answer']}'\")\n",
    "\n",
    "# Print the confidence score:\n",
    "print(f\"Confidence: {answers['score']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with a slightly different question\n",
    "When we ask _\"Who was Blaine?\"_ instead of _\"Who was **James** Blaine?\"_, the answers qwe got were quite different, especially in terms of score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1\n",
    "> Chunk 1: an American statesman and Republican politician (71.4%)<br/>\n",
    "> Chunk 2:  (49.7%)<br/>\n",
    "> Chunk 3: Grover Cleveland (77.7%)<br/>\n",
    "> Chunk 4: Republican of the late 19th century (68.6%)<br/>\n",
    "> Chunk 5: a leading Republican of the late 19th century (64.6%)<br/>\n",
    "\n",
    "\n",
    "**Final answers**<br/>\n",
    "> Q: 'who was Blaine?'<br/>\n",
    "> A: 'Grover Cleveland'<br/>\n",
    "> Confidence: 77.7%\n",
    "\n",
    "#### Version 2\n",
    "> Chunk 1: an American statesman and Republican politician (90.6%)<br/>\n",
    "> Chunk 2: President (77.3%)<br/>\n",
    "> Chunk 3: Grover Cleveland. (72.2%)<br/>\n",
    "> Chunk 4: Republican of the late 19th century (64.3%)<br/>\n",
    "> Chunk 5: a leading Republican of the late 19th century (73.9%)<br/>\n",
    "\n",
    "**Final answers**<br/>\n",
    "> Q: 'who was James Blaine?'<br/>\n",
    "> A: 'an American statesman and Republican politician'<br/>\n",
    "> Confidence: 90.6%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
